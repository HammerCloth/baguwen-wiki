<!--yml
category: 未分类
date: 0001-01-01 00:00:00
--->

# 579.SPARK 编程模型

> 原文：[https://zwmst.com/4267.html](https://zwmst.com/4267.html)

   [ *分布式* ](https://zwmst.com/%e5%88%86%e5%b8%83%e5%bc%8f)*[ <time datetime="2021-09-28T00:50:20+08:00"> 2021-09-27 </time> ](https://zwmst.com/4267.html)  ![](img/925966c786dc58895736a1a7370d5802.png)
Spark 应用程序从编写到提交、执行、输出的整个过程如图所示，图中描述的步骤如下：

1.  用户使用 SparkContext 提供的 API（常用的有 textFile、sequenceFile、runJob、stop 等）编写 Driver application 程序。此外 SQLContext、HiveContext 及 StreamingContext 对SparkContext 进行封装，并提供了 SQL、Hive 及流式计算相关的 API。
2.  使用SparkContext提交的用户应用程序，首先会使用BlockManager和BroadcastManager将任务的 Hadoop 配置进行广播。然后由 DAGScheduler 将任务转换为 RDD 并组织成 DAG，DAG 还将被划分为不同的 Stage。最后由 TaskScheduler 借助 ActorSystem 将任务提交给集群管理器（Cluster Manager）。
3.  集群管理器（ClusterManager）给任务分配资源，即将具体任务分配到Worker上，Worker创建 Executor 来处理任务的运行。Standalone、YARN、Mesos、EC2 等都可以作为 Spark的集群管理器。*