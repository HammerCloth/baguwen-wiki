# 爬虫

# 爬虫中如何实现一个调度器

> 原文：[https://q.shanyue.tech/server/%E7%88%AC%E8%99%AB/259.html](https://q.shanyue.tech/server/%E7%88%AC%E8%99%AB/259.html)

Issue

欢迎在 Gtihub Issue 中回答此问题: [Issue 259(opens new window)](https://github.com/shfshanyue/Daily-Question/issues/259)

# 如何实现一个分布式的爬虫

> 原文：[https://q.shanyue.tech/server/%E7%88%AC%E8%99%AB/260.html](https://q.shanyue.tech/server/%E7%88%AC%E8%99%AB/260.html)

Issue

欢迎在 Gtihub Issue 中回答此问题: [Issue 260(opens new window)](https://github.com/shfshanyue/Daily-Question/issues/260)

Author

回答者: [shfshanyue(opens new window)](https://github.com/shfshanyue)

可以通过 redis 实现一个分布式的 url 调度器 (Set)，多个分布式爬虫的爬取器从调度器中统一取地址进行爬取

# 爬虫如何实现一个去重器

> 原文：[https://q.shanyue.tech/server/%E7%88%AC%E8%99%AB/261.html](https://q.shanyue.tech/server/%E7%88%AC%E8%99%AB/261.html)

Issue

欢迎在 Gtihub Issue 中回答此问题: [Issue 261(opens new window)](https://github.com/shfshanyue/Daily-Question/issues/261)

Author

回答者: [shay-an(opens new window)](https://github.com/shay-an)

urlMap[url,hash(data)] urlSet[fullURL] urlMap 存不包含查询字符串的 url，data 相同也不存储 urlSet 存完整 url，即 Map 里没有查询到，则通过完整 url 去重

# 当写爬虫时，因爬取过多被禁掉 IP 怎么解决

> 原文：[https://q.shanyue.tech/server/%E7%88%AC%E8%99%AB/263.html](https://q.shanyue.tech/server/%E7%88%AC%E8%99%AB/263.html)

Issue

欢迎在 Gtihub Issue 中回答此问题: [Issue 263(opens new window)](https://github.com/shfshanyue/Daily-Question/issues/263)

Author

回答者: [shfshanyue(opens new window)](https://github.com/shfshanyue)

可以维护一个 IP 地址池，通过 Proxy 的方式去爬取网页

# 在服务端反爬虫有哪些策略

> 原文：[https://q.shanyue.tech/server/%E7%88%AC%E8%99%AB/468.html](https://q.shanyue.tech/server/%E7%88%AC%E8%99%AB/468.html)

更多描述

在服务端反爬/防爬有哪些策略

Issue

欢迎在 Gtihub Issue 中回答此问题: [Issue 468(opens new window)](https://github.com/shfshanyue/Daily-Question/issues/468)

Author

回答者: [shfshanyue(opens new window)](https://github.com/shfshanyue)

## 01 Referer: 当前页面的访问源

Referer 指当前请求页面的来源页面的地址，用以判断当前页面的访问源。一般用以反爬，比如**图片防盗链**通过判断 `Referer` 是否目标网站而对图片替换为禁止标志的图片。

## 02 User-Agent: 当前页面的用户代理，如浏览器等

User-Agent 指请求当前页面的用户代理，用以标识请求方环境，如浏览器等，如果没有这个字符串可以视为爬虫。同时为了避免伪造用户代理，可以对 User-Agent 进行**限流**，但同时也有随机生成 UA 的库

*   [random-useragent(opens new window)](https://github.com/skratchdot/random-useragent)

## 03 Rate-Limit: 限流

*   对 IP 地址进行限流
*   对 UA 进行限流