<!--yml
category: 未分类
date: 0001-01-01 00:00:00
-->

# 909.Redis 有两种类型分区。

> 原文：[https://zwmst.com/5086.html](https://zwmst.com/5086.html)

   [ *数据库* ](https://zwmst.com/%e6%95%b0%e6%8d%ae%e5%ba%93)*[ <time datetime="2021-10-16T03:12:29+08:00"> 2021-10-15 </time> ](https://zwmst.com/5086.html)  最简单的分区方式是按范围分区，就是映射一定范围的对象到特定的 Redis 实例。
比如，ID 从 0 到 10000 的用户会保存到实例 R0，ID 从 10001 到 20000 的用户会保存到 R1，以此类推。
这种方式是可行的，并且在实际中使用，不足就是要有一个区间范围到实例的映射表。这个表要被管理，同时还需要各 种对象的映射表，通常对 Redis 来说并非是好的方法。
哈希分区：另外一种分区方法是 hash 分区。这对任何 key 都适用，也无需是object_name: 这种形式，像下面描述的一样简单：
用一个 hash 函数将 key 转换为一个数字，比如使用 crc32 hash 函数。对 keyfoobar 执行 crc32(foobar) 会输出类似 93024922 的整数。
对这个整数取模，将其转化为 0-3 之间的数字，就可以将这个整数映射到 4 个Redis 实例中的一个了。93024922 % 4 = 2，就是说 key foobar 应该被存到 R2实例中。注意：取模操作是取除的余数，通常在多种编程语言中用 % 操作符实现。
实际上，上面的集群模式还存在两个问题：

1.  扩容问题：
    因为使用了一致性哈稀进行分片，那么不同的 key 分布到不同的 Redis-Server 上，当我们需要扩容时需要增加机器到分片列表中，这时候会使得同样的 key 算出来落到跟原来不同的机器上，这样如果要取某一个值，会出现取不到的情况，对于这种情况，Redis 的作者提出了一种名为 Pre-Sharding 的方式：Pre-Sharding 方法是将每一个台物理机上，运行多个不同断口的 Redis 实例，假如有三个物理机，每个物理机运行三个 Redis 实际，那么我们的分片列表中实际有 9 个 Redis 实例，当我们需要扩容时，增加一台物理机，步骤如下：
    1.  在新的物理机上运行 Redis-Server；
    2.  该 Redis-Server 从属于 (slaveof) 分片列表中的某一 Redis-Server（假设叫 RedisA）；
    3.  等主从复制 (Replication) 完成后，将客户端分片列表中 RedisA 的IP 和端口改为新物理机上 Redis-Server 的 IP 和端口；
    4.  停止 RedisA。
        这样相当于将某一 Redis-Server 转移到了一台新机器上。Prd-Sharding 实际上是一种在线扩容的办法，但还是很依赖 Redis 本身的复制功能的，如果主库快照数据文件过大，这个复制的过程也会很久，同时会给主库带来压力。所以做这个拆分的过程最好选择为业务访问低峰时段进行。
2.  单点故障问题：
    还是用到 Redis 主从复制的功能，两台物理主机上分别都运行有 Redis-Server，其中一个 Redis-Server 是另一个的从库，采用双机热备技术，客户端通过虚拟 IP 访问主库的物理 IP，当主库宕机时，切换到从库的物理 IP。只是事后修复主库时，应该将之前的从库改为主库（使用命令 slaveof no
    one），主库变为其从库（使命令 slaveof IP PORT），这样才能保证修复期间新增数据的一致性。*